{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 15\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "\n",
    "word2vec = {}\n",
    "with open('glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return \" \".join([item.lower() for item in text.split() if item not in stop])\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([\"\" if ord(i) < 32 or ord(i) > 126 else i for i in text])\n",
    "\n",
    "def process_train_data(df, filename):\n",
    "    \n",
    "    df = df[df.gold_label != \"-\"]\n",
    "    df['gold_label'] = df['gold_label'].map({val: i for i, val in enumerate(df['gold_label'].unique())})\n",
    "    df['sentence1'] = df['sentence1'].apply(remove_non_ascii)\n",
    "    df['sentence1'] = df['sentence1'].apply(normalize_text)\n",
    "    df['sentence1'] = df['sentence1'].apply(remove_stop_words)\n",
    "    df['sentence1'] = df['sentence1'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "    df['sentence2'] = df['sentence2'].apply(remove_non_ascii)\n",
    "    df['sentence2'] = df['sentence2'].apply(normalize_text)\n",
    "    df['sentence2'] = df['sentence2'].apply(remove_stop_words)\n",
    "    df['sentence2'] = df['sentence2'].str.replace('[^\\w\\s]','')\n",
    "    texts = df['sentence1'].values + df['sentence2'].values\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "    \n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    word2idx = tokenizer.word_index\n",
    "    \n",
    "    sentence1 = tokenizer.texts_to_sequences(df['sentence1'])\n",
    "    sentence2 = tokenizer.texts_to_sequences(df['sentence2'])\n",
    "    \n",
    "    sentence1 = pad_sequences(sentence1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    sentence2 = pad_sequences(sentence2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word2idx.items():\n",
    "          if i < MAX_VOCAB_SIZE:\n",
    "            embedding_vector = word2vec.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    sentence1_embeddings = []\n",
    "    for sentence in sentence1:\n",
    "        temp = []\n",
    "        for val in sentence:\n",
    "            temp.append(embedding_matrix[val])\n",
    "        sentence1_embeddings.append(np.asarray(temp))\n",
    "    sentence1_embeddings = np.asarray(sentence1_embeddings)\n",
    "    \n",
    "    sentence2_embeddings = []\n",
    "    for sentence in sentence2:\n",
    "        temp = []\n",
    "        for val in sentence:\n",
    "            temp.append(embedding_matrix[val])\n",
    "        sentence2_embeddings.append(np.asarray(temp))\n",
    "    sentence2_embeddings = np.asarray(sentence2_embeddings)\n",
    "    \n",
    "    with open('sentence1_embedd_%s'%filename,'w') as outfile:\n",
    "        np.save(outfile, sentence1_embeddings.reshape(sentence1_embeddings.shape[0],-1))\n",
    "        \n",
    "    with open('sentence2_embedd_%s'%filename,'w') as outfile:\n",
    "        np.save(outfile, sentence2_embeddings.reshape(sentence1_embeddings.shape[0],-1))\n",
    "        \n",
    "    labels = np.asarray(df['gold_label'])\n",
    "    labels.astype(np.float32)\n",
    "    \n",
    "    true_label = np.zeros((labels.shape[0], 3))\n",
    "    true_label[np.arange(labels.shape[0]), labels] = 1\n",
    "\n",
    "    with open('label_%s'%filename, 'w') as outfile:\n",
    "        np.save(outfile, true_label)\n",
    "        \n",
    "    return tokenizer, embedding_matrix\n",
    "        \n",
    "        \n",
    "def process_test_data(df, filename, tokenizer, embedding_matrix):\n",
    "    \n",
    "    df = df[df.gold_label != \"-\"]\n",
    "    df['gold_label'] = df['gold_label'].map({val: i for i, val in enumerate(df['gold_label'].unique())})\n",
    "    df['sentence1'] = df['sentence1'].apply(remove_non_ascii)\n",
    "    df['sentence1'] = df['sentence1'].apply(normalize_text)\n",
    "    df['sentence1'] = df['sentence1'].apply(remove_stop_words)\n",
    "    df['sentence1'] = df['sentence1'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "    df['sentence2'] = df['sentence2'].apply(remove_non_ascii)\n",
    "    df['sentence2'] = df['sentence2'].apply(normalize_text)\n",
    "    df['sentence2'] = df['sentence2'].apply(remove_stop_words)\n",
    "    df['sentence2'] = df['sentence2'].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    ## tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "    # texts = df['sentence1'].values + df['sentence2'].values\n",
    "    # tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # word2idx = tokenizer.word_index\n",
    "    \n",
    "    sentence1 = tokenizer.texts_to_sequences(df['sentence1'])\n",
    "    sentence2 = tokenizer.texts_to_sequences(df['sentence2'])\n",
    "    \n",
    "    sentence1 = pad_sequences(sentence1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    sentence2 = pad_sequences(sentence2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    \"\"\"\n",
    "    num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word2idx.items():\n",
    "          if i < MAX_VOCAB_SIZE:\n",
    "            embedding_vector = word2vec.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence1_embeddings = []\n",
    "    for sentence in sentence1:\n",
    "        temp = []\n",
    "        for val in sentence:\n",
    "            temp.append(embedding_matrix[val])\n",
    "        sentence1_embeddings.append(np.asarray(temp))\n",
    "    sentence1_embeddings = np.asarray(sentence1_embeddings)\n",
    "    \n",
    "    sentence2_embeddings = []\n",
    "    for sentence in sentence2:\n",
    "        temp = []\n",
    "        for val in sentence:\n",
    "            temp.append(embedding_matrix[val])\n",
    "        sentence2_embeddings.append(np.asarray(temp))\n",
    "    sentence2_embeddings = np.asarray(sentence2_embeddings)\n",
    "    \n",
    "    with open('sentence1_embedd_%s'%filename,'w') as outfile:\n",
    "        np.save(outfile, sentence1_embeddings.reshape(sentence1_embeddings.shape[0],-1))\n",
    "        \n",
    "    with open('sentence2_embedd_%s'%filename,'w') as outfile:\n",
    "        np.save(outfile, sentence2_embeddings.reshape(sentence1_embeddings.shape[0],-1))\n",
    "        \n",
    "    labels = np.asarray(df['gold_label'])\n",
    "    labels.astype(np.float32)\n",
    "    \n",
    "    true_label = np.zeros((labels.shape[0], 3))\n",
    "    true_label[np.arange(labels.shape[0]), labels] = 1\n",
    "    \n",
    "    with open('label_%s'%filename, 'w') as outfile:\n",
    "        np.save(outfile, true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train'\n",
    "with open(filename) as f:\n",
    "    data = pd.DataFrame(json.loads(line) for line in f)\n",
    "    df = data[['gold_label', 'sentence1', 'sentence2']]\n",
    "tokenizer, embedding_matrix = process_train_data(df, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test'\n",
    "with open(filename) as f:\n",
    "    data = pd.DataFrame(json.loads(line) for line in f)\n",
    "    df = data[['gold_label', 'sentence1', 'sentence2']]\n",
    "process_test_data(df, filename, tokenizer, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'dev'\n",
    "with open(filename) as f:\n",
    "    data = pd.DataFrame(json.loads(line) for line in f)\n",
    "    df = data[['gold_label', 'sentence1', 'sentence2']]\n",
    "process_test_data(df, filename, tokenizer, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load('./label_train')\n",
    "sentence1_train = np.load('./sentence1_embedd_train')\n",
    "sentence2_train = np.load('./sentence2_embedd_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.load('./label_test')\n",
    "sentence1_test = np.load('./sentence1_embedd_test')\n",
    "sentence2_test = np.load('./sentence2_embedd_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels = np.load('./label_dev')\n",
    "sentence1_dev = np.load('./sentence1_embedd_dev')\n",
    "sentence2_dev = np.load('./sentence2_embedd_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbatch(batch_size):\n",
    "    while True:\n",
    "        for i in range(len(train_labels)// batch_size):\n",
    "            index = i * batch_size\n",
    "            index_end = index + batch_size\n",
    "            yield (sentence1_train[index: index_end], sentence2_train[index: index_end], train_labels[index: index_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_input = 15 * 50\n",
    "nn1_hidden1 = 500\n",
    "nn1_output = 100\n",
    "# nn1_output = 300\n",
    "\n",
    "nn2_input = nn1_output * 2\n",
    "nn2_hidden1 = 100\n",
    "# nn2_hidden1 = 50\n",
    "nn2_output = 3\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "batch_size = 128\n",
    "num_steps = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = tf.placeholder(tf.float32, shape=(None, nn1_input))\n",
    "sentence2 = tf.placeholder(tf.float32, shape=(None, nn1_input))\n",
    "\n",
    "output = tf.placeholder(tf.float32, shape=(None, nn2_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_weights = {\n",
    "    \"nn1_w1\" : tf.Variable(tf.random_normal([nn1_input, nn1_hidden1])),\n",
    "    \"nn1_w2\" : tf.Variable(tf.random_normal([nn1_hidden1, nn1_output])),\n",
    "    \n",
    "    \"nn2_w1\" : tf.Variable(tf.random_normal([nn2_input, nn2_hidden1])),\n",
    "    \"nn2_w2\" : tf.Variable(tf.random_normal([nn2_hidden1, nn2_output]))\n",
    "}\n",
    "\n",
    "nn1_biases = {\n",
    "    \"nn1_b1\" : tf.Variable(tf.random_normal([nn1_hidden1])),\n",
    "    \"nn1_b2\" : tf.Variable(tf.random_normal([nn1_output])),\n",
    "    \n",
    "    \"nn2_b1\" : tf.Variable(tf.random_normal([nn2_hidden1])),\n",
    "    \"nn2_b2\" : tf.Variable(tf.random_normal([nn2_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn1_encode(x):\n",
    "    val = tf.nn.sigmoid(tf.matmul(x, nn1_weights[\"nn1_w1\"]) + nn1_biases[\"nn1_b1\"])\n",
    "    return tf.nn.sigmoid(tf.matmul(val, nn1_weights[\"nn1_w2\"]) + nn1_biases[\"nn1_b2\"])\n",
    "\n",
    "e1 = nn1_encode(sentence1)\n",
    "e2 = nn1_encode(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2_input = tf.concat([e1, e2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn2_encode(x):\n",
    "    val = tf.nn.sigmoid(tf.matmul(x, nn1_weights[\"nn2_w1\"]) + nn1_biases[\"nn2_b1\"])\n",
    "    return tf.nn.sigmoid(tf.matmul(val, nn1_weights[\"nn2_w2\"]) + nn1_biases[\"nn2_b2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn2_encode(nn2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=predictions, labels=output))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(output, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_step = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    z = getbatch(batch_size)\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_sentence1, batch_sentence2, batch_y = z.next()\n",
    "        sess.run(train_op, feed_dict={sentence1: batch_sentence1,sentence2: batch_sentence2, output: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={sentence1: batch_sentence1,sentence2: batch_sentence2, output: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Step Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "       sess.run(accuracy, feed_dict={sentence1: sentence1_test,sentence2: sentence2_test, output: test_labels}))\n",
    "    \n",
    "    print(\"Dev Accuracy:\", \\\n",
    "       sess.run(accuracy, feed_dict={sentence1: sentence1_dev,sentence2: sentence2_dev, output: dev_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
